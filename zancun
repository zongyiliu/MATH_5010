
		\underline{Inference about $\beta_1$} We often want to perform tests about the \emph{slope}:
		$
		H_0:\ \beta_1 = 0
		\quad \text{versus} \quad
		H_1:\ \beta_1 \neq 0.
		$ Under the null hypothesis there is \emph{no linear relationship} between $Y$ and $x$, i.e., the means of the probability distributions of $Y$ at all levels of $x$ are equal, i.e.,
		$
		\mathbb{E}(Y \mid x) = \beta_0, \quad \text{for all } x.
		$
		In fact, the sampling distribution of $\hat{\beta}_1$ is given by
		$
		\hat{\beta}_1 \sim N\!\left( \beta_1, \frac{\sigma^2}{S_x^2} \right).
		$ Need to show that $\hat{\beta}_1$ is normally distributed,
		$
		\mathbb{E}(\hat{\beta}_1) = \beta_1,
		\qquad
		\mathrm{Var}(\hat{\beta}_1) = \frac{\sigma^2}{S_x^2}.
		$ {Result.}
		When $Z_1,\ldots,Z_k$ are independent normal random variables, the linear combination
		$
		a_1 Z_1 + \cdots + a_k Z_k
		$
		is also normally distributed. Since $\hat{\beta}_1$ is a linear combination of the $Y_i$'s and each $Y_i$ is an independent normally distributed random variable, then $\hat{\beta}_1$ is also normally distributed. We can write
		$
		\hat{\beta}_1 = \sum_{i=1}^n w_i Y_i
		\quad \text{where} \quad
		w_i = \frac{x_i - \bar{x}}{S_x^2},
		\qquad i = 1,\ldots,n.
		$ Thus,
		$
		\sum_{i=1}^n w_i = 0,
		\qquad
		\sum_{i=1}^n x_i w_i = 1,
		\qquad
		\sum_{i=1}^n w_i^2 = \frac{1}{S_x^2}.
		$ {Variance for the estimated slope:}
		There are three aspects of the scatter plot that affect the variance of the regression slope:
		\begin{itemize}
			\item The spread around the regression line $(\sigma^2)$ --- less scatter around the line means the slope will be more consistent from sample to sample.
			\item The spread of the $x$ values $\bigl( \sum_{i=1}^n (x_i - \bar{x})^2 / n \bigr)$ --- a large variance of $x$ provides a more stable regression.
			\item The sample size $n$ --- having a larger sample size gives more consistent estimates.
		\end{itemize}
		
		{Estimated variance:}
		When $\sigma^2$ is unknown we replace it with
		$
		\hat{\sigma}^2
		= \frac{\sum_{i=1}^n (Y_i - \hat{Y}_i)^2}{n-2}
		= \frac{\sum_{i=1}^n e_i^2}{n-2}.
		$ Plugging this into the equation for $\mathrm{Var}(\hat{\beta}_1)$ we get
		$
		\mathrm{se}^2(\hat{\beta}_1)
		= \frac{\hat{\sigma}^2}{S_x^2}.
		$ Recall: \emph{Standard error} $\mathrm{se}(\hat{\theta})$ of an estimator $\hat{\theta}$ is used to refer to an estimate of its standard deviation. {Result:} For the normal error regression model:
		$
		\frac{\mathrm{SSE}}{\sigma^2}
		= \frac{\sum_{i=1}^n (Y_i - \hat{Y}_i)^2}{\sigma^2}
		\sim \chi^2_{n-2},
		$
		and is independent of $\hat{\beta}_0$ and $\hat{\beta}_1$.
		
		{Studentized statistic:}
		Since $\hat{\beta}_1$ is normally distributed, the standardized statistic:
		$
		\frac{\hat{\beta}_1 - \beta_1}{\sqrt{\mathrm{Var}(\hat{\beta}_1)}}
		\sim N(0,1).$If we replace $\mathrm{Var}(\hat{\beta}_1)$ by its estimate we get the \emph{studentized statistic}:
		$
		\frac{\hat{\beta}_1 - \beta_1}{\mathrm{se}(\hat{\beta}_1)}
		\sim t_{n-2}.
		$ Recall: Suppose that $Z \sim N(0,1)$ and $W \sim \chi^2_p$ where $Z$ and $W$ are independent. Then,
		$
		\frac{Z}{\sqrt{W/p}} \sim t_p,
		$
		the $t$-distribution with $p$ degrees of freedom.
		
		{Hypothesis testing:}
		To test
		$
		H_0:\ \beta_1 = 0
		\qquad \text{versus} \qquad
		H_a:\ \beta_1 \neq 0,
		$
		use the test-statistic
		$
		T = \frac{\hat{\beta}_1}{\mathrm{se}(\hat{\beta}_1)}.
		$ We reject $H_0$ when the observed value of $|T|$, i.e., $|t_{\mathrm{obs}}|$, is large. Thus, given level $(1-\alpha)$, we reject $H_0$ if
		$
		|t_{\mathrm{obs}}| > t_{1-\alpha/2,\,n-2},
		$
		where $t_{1-\alpha/2,\,n-2}$ denotes the $(1-\alpha/2)$-quantile of the $t_{n-2}$-distribution, i.e.,
		$
		1 - \frac{\alpha}{2}
		= \mathbb{P}(T \le t_{1-\alpha/2,\,n-2}).
		$
		
		{P-value:}
		The $p$-value is the probability of obtaining a test statistic at least as extreme as the one that was actually observed, assuming that the null hypothesis is true. The $p$-value depends on $H_1$ (one-sided/two-sided). In our case, we compute $p$-values using a $t_{n-2}$-distribution. Thus,
		$
		p\text{-value}
		= \mathbb{P}_{H_0}\!\left( |T| > |t_{\mathrm{obs}}| \right).
		$ If we know the $p$-value then we can decide to accept/reject $H_0$ (versus $H_1$) at any given $\alpha \in (0,1)$.
		
		{Confidence interval:} A \emph{confidence interval} (CI) is a kind of interval estimator of a population parameter and is used to indicate the reliability of an estimator. Using the sampling distribution of $\hat{\beta}_1$ we can make the following probability statement:
		$
		\mathbb{P}\!\left(
		t_{\alpha/2,\,n-2}
		\le
		\frac{\hat{\beta}_1 - \beta_1}{\mathrm{se}(\hat{\beta}_1)}
		\le
		t_{1-\alpha/2,\,n-2}
		\right)
		= 1 - \alpha.
		$ Equivalently,
		$
		\mathbb{P}\!\left(
		\hat{\beta}_1 - t_{1-\alpha/2,\,n-2}\,\mathrm{se}(\hat{\beta}_1)
		\le
		\beta_1
		\le
		\hat{\beta}_1 + t_{1-\alpha/2,\,n-2}\,\mathrm{se}(\hat{\beta}_1)
		\right)
		= 1 - \alpha.
		$
		
		Thus, a $(1-\alpha)$ confidence interval for $\beta_1$ is
		$
		\bigl[
		\hat{\beta}_1 - t_{1-\alpha/2,\,n-2}\,\mathrm{se}(\hat{\beta}_1),
		\;
		\hat{\beta}_1 + t_{1-\alpha/2,\,n-2}\,\mathrm{se}(\hat{\beta}_1)
		\bigr],
		$
		as $t_{1-\alpha/2,\,n-2} = -t_{\alpha/2,\,n-2}$.
		\underline{Prediction interval} A CI for a \emph{future observation} is called a \emph{prediction interval}.
		Consider the prediction of a new observation $Y$ corresponding to a given level
		$x$ of the predictor.
		
		Suppose $x=x_h$ and the new observation is denoted $Y_{h(\text{new})}$.
		Note that $E(\hat Y_h)$ is the mean of the distribution of $Y\mid X=x_h$.
		$Y_{h(\text{new})}$ represents the prediction of an \emph{individual outcome}
		drawn from the distribution of $Y\mid X=x_h$, i.e.,
		$
		Y_{h(\text{new})}=\beta_0+\beta_1 x_h+\varepsilon_{\text{new}},
		$
		where $\varepsilon_{\text{new}}$ is independent of our data.
		
		\begin{itemize}
			\item The \emph{point estimate} will be the \emph{same} for both.
			
			\item However, the variance is \emph{larger} when predicting an individual outcome
			due to the \emph{additional variation} of an individual about the mean.
			
			\item When constructing prediction limits for $Y_{h(\text{new})}$ we must take into
			consideration two sources of variation:
			\begin{itemize}
				\item Variation in the \emph{mean} of $Y$.
				\item Variation around the mean.
			\end{itemize}
			
			\item The sampling distribution of the studentized statistic:
			$
			\frac{Y_{h(\text{new})}-\hat Y_h}{\se\!\left(Y_{h(\text{new})}-\hat Y_h\right)}
			\sim t_{n-2}.
			$
		\end{itemize}
		
		All inference regarding $Y_{h(\text{new})}$ are carried out using the $t$-distribution:
		$
		\Var\!\left(Y_{h(\text{new})}-\hat Y_h\right)
		=
		\Var(Y_{h(\text{new})})+\Var(\hat Y_h)
		=
		\sigma^2\!\left\{
		1+\frac{1}{n}+\frac{(x_h-\bar x)^2}{S_x^2}
		\right\}.
		$
		
		Thus,
		$
		\se_{\text{pred}}
		=
		\se\!\left(Y_{h(\text{new})}-\hat Y_h\right)
		=
		\tilde\sigma^2
		\left\{
		1+\frac{1}{n}+\frac{(x_h-\bar x)^2}{S_x^2}
		\right\}.
		$
		
		Using this result, a $(1-\alpha)$ \emph{prediction interval} for a new observation
		$Y_{h(\text{new})}$ is
		$
		\hat Y_h \pm t_{1-\alpha/2,n-2}\,\se_{\text{pred}}.
		$
		
		\subsection*{2.4.5\quad Inference about both $\beta_0$ and $\beta_1$ simultaneously}
		
		Suppose that $\beta_0^*$ and $\beta_1^*$ are given numbers and we are interested in
		testing the following hypothesis:
		$
		H_0:\ \beta_0=\beta_0^* \ \text{and}\ \beta_1=\beta_1^*
		\qquad \text{versus} \qquad
		H_1:\ \text{at least one is different}.
		$
		
		We shall derive the likelihood ratio test for (10).
		
		The likelihood function (8), when maximized under the unconstrained space yields the
		MLEs $\hat\beta_0,\hat\beta_1,\hat\sigma^2$.
		
		Under the constrained space, $\beta_0$ and $\beta_1$ are fixed at
		$\beta_0^*$ and $\beta_1^*$, and so
		$
		\hat\sigma_0^2
		=
		\frac{1}{n}\sum_{i=1}^n (Y_i-\beta_0^*-\beta_1^*x_i)^2.
		$
		
		The likelihood statistic reduces to
		$
		\Lambda(\mathbf Y,\mathbf x)
		=
		\frac{\sup_{\sigma^2} L(\beta_0^*,\beta_1^*,\sigma^2)}
		{\sup_{\beta_0,\beta_1,\sigma^2} L(\beta_0,\beta_1,\sigma^2)}
		=
		\left(\frac{\hat\sigma^2}{\hat\sigma_0^2}\right)^{n/2}
		=
		\left[
		\frac{\sum_{i=1}^n (Y_i-\hat\beta_0-\hat\beta_1 x_i)^2}
		{\sum_{i=1}^n (Y_i-\beta_0^*-\beta_1^* x_i)^2}
		\right]^{n/2}.
		$
		
		The LRT procedure specifies rejecting $H_0$ when
		$
		\Lambda(\mathbf Y,\mathbf x)\le k,
		$
		for some $k$, chosen given the level condition.
		
		\textbf{Exercise.} Show that
		$
		\sum_{i=1}^n (Y_i-\beta_0^*-\beta_1^*x_i)^2=S^2+Q^2,
		$
		where
		$
		S^2=\sum_{i=1}^n (Y_i-\hat\beta_0-\hat\beta_1 x_i)^2,
		$
		and
		$
		Q^2
		=
		n(\hat\beta_0-\beta_0^*)^2
		+
		\left(\sum_{i=1}^n x_i^2\right)(\hat\beta_1-\beta_1^*)^2
		+
		2n\bar x(\hat\beta_0-\beta_0^*)(\hat\beta_1-\beta_1^*).
		$
		
		Thus,
		$
		\Lambda(\mathbf Y,\mathbf x)
		=
		\left[\frac{S^2}{S^2+Q^2}\right]^{n/2}
		=
		\left[1+\frac{Q^2}{S^2}\right]^{-n/2}.
		$
		
		It can be seen that this is equivalent to rejecting $H_0$ when
		$Q^2/S^2\ge k'$, which is equivalent to
		$
		U^2:=\frac{1}{2}\frac{Q^2}{\tilde\sigma^2}\ge \gamma.
		$
		
		\textbf{Exercise.} Show that, under $H_0$,
		$\displaystyle \frac{Q^2}{\sigma^2}\sim\chi_2^2$.
		Also show that $Q^2$ and $S^2$ are independent.
		
		We know that $\displaystyle S^2/\sigma^2\sim\chi_{n-2}^2$.
		Thus, under $H_0$,
		$
		U^2\sim F_{2,n-2},
		$
		and hence
		$
		\gamma=F_{2,n-2}^{-1}(1-\alpha).
		$
		
		
		
		\clearpage
		
