
		\underline{Sampling Distribution (抽样分布).}
		设 $X_1,\dots,X_n \stackrel{\text{i.i.d.}}{\sim} P_\theta$，其中 $\theta\in\Omega\subset\mathbb R^k$。
		令统计量
		$
		T=\varphi(X_1,\dots,X_n),
		$
		并记其分布为 $T\sim F_\theta$（允许依赖于 $\theta$）。
		
		\medskip
		\noindent
		\textbf{定义：}
		在参数 $\theta$ 固定时，统计量 $T$ 的分布称为 $T$ 的 \emph{sampling distribution}，
		其累积分布函数为 $F_\theta$。
		抽样分布刻画了统计量可能取到的值及其概率。
		
		\medskip
		\noindent
		\textbf{例：}
		若 $X_1,\dots,X_n \stackrel{\text{i.i.d.}}{\sim} N(\mu,\sigma^2)$，则样本均值
		$
		\bar X_n \sim N\!\left(\mu,\frac{\sigma^2}{n}\right).
		$
		
		\underline{Gamma Function.}
		Gamma 函数定义为
		$
		\Gamma(\alpha)=\int_0^\infty x^{\alpha-1}e^{-x}\,dx,\qquad \alpha>0.
		$
		
		\noindent
		\textbf{基本性质：}
		$
		\Gamma(\alpha+1)=\alpha\Gamma(\alpha),\qquad 
		\Gamma(n)=(n-1)!,\ n\in\mathbb N,
		$
		且
		$
		\Gamma(1)=1,\qquad \Gamma\!\left(\tfrac12\right)=\sqrt{\pi}.
		$
		
		\underline{Gamma Distribution.}
		随机变量 $X\sim\mathrm{Gamma}(\alpha,\lambda)$（$\alpha>0,\lambda>0$）的密度为
		$
		f(x\mid\alpha,\lambda)
		=\frac{\lambda^\alpha}{\Gamma(\alpha)}e^{-\lambda x}x^{\alpha-1}I_{(0,\infty)}(x).
		$
		
		\noindent
		其中 $\alpha$ 为 \emph{shape} 参数，$\lambda$ 为 \emph{scale}（更准确地说是 rate）参数。
		
		\underline{Scale Property.}
		若 $X\sim\mathrm{Gamma}(\alpha,\lambda)$，则
		$
		\lambda X\sim\mathrm{Gamma}(\alpha,1),
		\qquad
		cX\sim\mathrm{Gamma}\!\left(\alpha,\frac{\lambda}{c}\right),\ c>0.
		$
		
		\underline{Reproductive Property.}
		若 $X_i\sim\mathrm{Gamma}(\alpha_i,\lambda)$ 相互独立，则
		$
		S_n:=\sum_{i=1}^n X_i \sim \mathrm{Gamma}\!\left(\sum_{i=1}^n\alpha_i,\lambda\right).
		$
		
		\underline{Moments.}
		若 $X\sim\mathrm{Gamma}(\alpha,\lambda)$，则
		$
		\mathbb E[X]=\frac{\alpha}{\lambda},
		\qquad
		\mathrm{Var}(X)=\frac{\alpha}{\lambda^2}.
		$
		
		更一般地，对任意正整数 $k$，
		$
		\mathbb E[X^k]
		=\frac{\Gamma(\alpha+k)}{\lambda^k\Gamma(\alpha)}
		=\frac{\prod_{i=1}^k(\alpha+i-1)}{\lambda^k}.
		$
		
		\noindent
		以上计算基于恒等式
		$
		\int_0^\infty e^{-\lambda x}x^{\alpha+k-1}\,dx
		=\frac{\Gamma(\alpha+k)}{\lambda^{\alpha+k}}.
		$
		
		\underline{Chi-squared Distribution.}
		$\chi^2$ 分布是 Gamma 分布的一个重要特例。
		
		\underline{$\chi^2_1$ Distribution.}
		设 $Z\sim N(0,1)$，定义
		$
		W:=Z^2.
		$
		则 $W$ 的分布称为 $\chi^2_1$ 分布，且
		$
		\chi^2_1 \sim \mathrm{Gamma}\!\left(\tfrac12,\tfrac12\right).
		$
		
		\underline{$\chi^2_d$ Distribution.}
		设 $Z_1,\dots,Z_d \stackrel{\text{i.i.d.}}{\sim} N(0,1)$，定义
		$
		W_d:=\sum_{i=1}^d Z_i^2.
		$
		则 $W_d$ 服从自由度为 $d$ 的卡方分布，记作
		$
		W_d\sim\chi^2_d,
		\qquad
		\chi^2_d \sim \mathrm{Gamma}\!\left(\tfrac d2,\tfrac12\right).
		$
		
		\underline{Additivity (Reproductive Property).}
		若 $X_i\sim\chi^2_{m_i}$ 相互独立，则
		$
		\sum_{i=1}^k X_i \sim \chi^2_{\sum_{i=1}^k m_i}.
		$
		
		\underline{Moments.}
		若 $X\sim\chi^2_m$，则
		$
		\mathbb E[X]=m,
		\qquad
		\mathrm{Var}(X)=2m.
		$
		
		\underline{Geometric Interpretation.}
		若 $Z_1,\dots,Z_d \stackrel{\text{i.i.d.}}{\sim} N(0,1)$，
		则
		$
		R^2=\sum_{i=1}^d Z_i^2 \sim \chi^2_d,
		$
		其中 $R$ 是随机向量 $(Z_1,\dots,Z_d)$ 到原点的欧氏距离。
		
		\underline{Confidence Intervals (CIs).}
		置信区间用于量化对未知参数 $\theta$（或其函数 $g(\theta)$）的估计不确定性。
		我们希望构造一个随机区间 $(A,B)$，使其在重复抽样意义下有高概率包含真值。
		
		\underline{Definition (Level / Coefficient).}
		设 $X_n=(X_1,\dots,X_n)$ 来自分布族 $P_\theta$，其中 $\theta\in\Omega\subset\mathbb R^k$。
		欲估计 $g(\theta)$。若统计量 $A\le B$ 满足对所有 $\theta$，
		$
		\mathbb P_\theta\!\bigl(A\le g(\theta)\le B\bigr)\ge 1-\alpha,\qquad \alpha\in(0,1),
		$
		则随机区间 $(A,B)$ 称为 $g(\theta)$ 的 $(1-\alpha)$ 置信区间（置信水平/系数）。
		若上式对所有 $\theta$ 都取等号，则称该 CI 为 \emph{exact}（精确置信区间）。
		
		\underline{Standard Method: Pivots (枢轴量法).}
		构造 CI 的常用套路是找一个 \emph{pivot}（枢轴量）：
		$
		\Psi\bigl(X_n,g(\theta)\bigr),
		$
		使得其在参数为 $\theta$ 时的分布 \emph{不依赖于 $\theta$}，且分布已知。
		设该 pivot 的分布函数为 $G$，并记其 $\beta$ 分位数为 $q(G;\beta)$，即
		$
		\mathbb P_\theta\!\left(\Psi\bigl(X_n,g(\theta)\bigr)\le q(G;\beta)\right)=\beta.
		$
		选取 $0\le \beta_1,\beta_2\le \alpha$ 且 $\beta_1+\beta_2=\alpha$，则
		$
		\mathbb P_\theta\!\left(q(G;\beta_1)\le \Psi\bigl(X_n,g(\theta)\bigr)\le q(G;1-\beta_2)\right)=1-\alpha.
		$
		把上述不等式对 $g(\theta)$（或 $\theta$）解出来，得到 $(1-\alpha)$ 的置信集合/区间。
		\emph{常见选择是等分尾部：} $\beta_1=\beta_2=\alpha/2$（通常可得到最短的双侧 CI）。
		
		\underline{Example 1: Normal Mean, $\sigma$ Known.}
		若 $X_1,\dots,X_n\stackrel{\text{i.i.d.}}{\sim}N(\mu,\sigma^2)$ 且 $\sigma$ 已知，
		$
		\bar X=\frac1n\sum_{i=1}^n X_i,\qquad \bar X\sim N\!\left(\mu,\frac{\sigma^2}{n}\right).
		$
		标准化得到 pivot
		$
		\Psi(X_n,\mu)=\frac{\bar X-\mu}{\sigma/\sqrt n}=\frac{\sqrt n(\bar X-\mu)}{\sigma}\sim N(0,1).
		$
		令 $z_\beta$ 为 $N(0,1)$ 的 \emph{upper} $\beta$ 分位数（$\mathbb P(Z>z_\beta)=\beta$），
		则双侧 $(1-\alpha)$ CI 满足
		$
		\mathbb P_\mu\!\left(-z_{\alpha/2}\le \frac{\sqrt n(\bar X-\mu)}{\sigma}\le z_{\alpha/2}\right)=1-\alpha,
		$
		等价于
		$
		\mathbb P_\mu\!\left(\bar X-\frac{\sigma}{\sqrt n}z_{\alpha/2}\le \mu\le
		\bar X+\frac{\sigma}{\sqrt n}z_{\alpha/2}\right)=1-\alpha,
		$
		因此
		$
		\mathrm{CI}_{1-\alpha}(\mu)=\left[\bar X-\frac{\sigma}{\sqrt n}z_{\alpha/2},\ \bar X+\frac{\sigma}{\sqrt n}z_{\alpha/2}\right].
		$
		该 CI 长度为
		$
		L=2\frac{\sigma}{\sqrt n}z_{\alpha/2}.
		$
		\emph{权衡：} $\alpha$ 越小（置信度越高），$z_{\alpha/2}$ 越大，区间越宽；固定 $\alpha$ 时，
		$n$ 越大，长度按 $1/\sqrt n$ 缩短（样本量增至 $4$ 倍，长度减半）。
		
		\underline{Example 2: Normal Mean, $\sigma$ Unknown.}
		若仍有 $X_i\stackrel{\text{i.i.d.}}{\sim}N(\mu,\sigma^2)$ 但 $\sigma$ 未知，
		$
		s^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X)^2.
		$
		直接用 $\sigma$ 的 pivot 不可行（含 nuisance 参数）。用 $s$ 替代 $\sigma$：
		$
		\Psi(X_n,\mu)=\frac{\sqrt n(\bar X-\mu)}{s}.
		$
		利用正态样本性质可得该 pivot 服从 $t$ 分布（自由度 $n-1$）：
		$
		\frac{\sqrt n(\bar X-\mu)}{s}\sim t_{n-1}.
		$
		由对称性，设 $q(t_{n-1};1-\alpha/2)$ 为上 $1-\alpha/2$ 分位数，则
		$
		\mathbb P_{\mu,\sigma^2}\!\left(-q(t_{n-1};1-\alpha/2)\le \frac{\sqrt n(\bar X-\mu)}{s}\le
		q(t_{n-1};1-\alpha/2)\right)=1-\alpha,
		$
		等价于
		$
		\mathrm{CI}_{1-\alpha}(\mu)=\left[\bar X-\frac{s}{\sqrt n}q(t_{n-1};1-\alpha/2),\ 
		\bar X+\frac{s}{\sqrt n}q(t_{n-1};1-\alpha/2)\right].
		$
		
		\underline{Asymptotic Pivots via CLT (大样本近似).}
		若 $X_1,\dots,X_n$ 为来自某分布 $F$ 的 i.i.d. 样本，满足
		$
		\mathbb E[X_1]=\mu,\qquad \mathrm{Var}(X_1)=\sigma^2,
		$
		则由 CLT，
		$
		\frac{\sqrt n(\bar X-\mu)}{\sigma}\ \overset{\text{approx}}{\sim}\ N(0,1)\qquad (n\ \text{大}).
		$
		若 $\sigma$ 已知，则得到近似 CI：
		$
		\mu \approx \bar X \pm \frac{\sigma}{\sqrt n}z_{\alpha/2}.
		$
		实际中 $\sigma$ 常未知，可用 $s$ 替代，仍得到近似 CI：
		$
		\mu \approx \left(\bar X-\frac{s}{\sqrt n}z_{\alpha/2},\ \bar X+\frac{s}{\sqrt n}z_{\alpha/2}\right),
		$
		但其真实覆盖率可能偏离 $1-\alpha$，并依赖于 $F$ 的形状与样本量 $n$。
		
		\underline{Exercise (Bernoulli, Large $n$).}
		若 $X_i\stackrel{\text{i.i.d.}}{\sim}\mathrm{Bernoulli}(\theta)$，则
		$
		\mathbb E[X_1]=\theta,\qquad \mathrm{Var}(X_1)=\theta(1-\theta).
		$
		令样本比例 $\hat\theta=\frac1n\sum_{i=1}^n X_i$，大样本下可用近似 pivot 构造 CI，得到（题中形式）
		$
		\left[\hat\theta-\sqrt{\frac{\hat\theta(1-\hat\theta)}{\,n-1\,}}\,z_{\alpha/2},\ 
		\hat\theta+\sqrt{\frac{\hat\theta(1-\hat\theta)}{\,n-1\,}}\,z_{\alpha/2}\right]
		$
		为 $\theta$ 的近似 $(1-\alpha)$ CI。
		
		\underline{Interpretation (如何正确解读).}
		令 $(A,B)$ 为参数 $\theta$ 的置信系数 $\gamma$（即 $\gamma=1-\alpha$）的置信区间，
		观察到具体数值 $(a,b)$ 后：
		\begin{itemize}
			\item \textbf{错误说法：} “$\theta$ 以概率 $\gamma$ 落在 $(a,b)$ 里。”
			\item \textbf{正确频率学派含义：} 在重复抽样下，随机区间 $(A(X_n),B(X_n))$ 以概率 $\gamma$
			覆盖真值 $\theta$：
			$
			\mathbb P_\theta\!\bigl(A(X_n)\le \theta\le B(X_n)\bigr)=\gamma.
			$
			\item 观测到 $(a,b)$ 之后，若不把 $\theta$ 当随机变量，就不能对事件 $\{\theta\in(a,b)\}$ 再赋“概率”；
			通常表述为“我们对 $\theta\in(a,b)$ 有置信度 $\gamma$”。
		\end{itemize}
		
		
		\textbf{$t$-Distribution.}

		
		\underline{Definition.}  分布可视为正态变量与独立卡方变量之比,
		设
		$
		Z\sim N(0,1),\qquad V\sim\chi^2_n,
		$
		且 $Z$ 与 $V$ 相互独立。定义
		$
		T=\frac{Z}{\sqrt{V/n}},
		$
		则称 $T$ 服从自由度为 $n$ 的 $t$ 分布，记作
		$
		T\sim t_n.
		$
		
		\underline{Symmetry.}
		若 $T\sim t_n$，则
		$
		T\overset{d}= -T,
		$
		因此 $t_n$ 分布关于 $0$ 对称，其密度函数为偶函数。
		
		\underline{Asymptotic Normality.}
		当 $n\to\infty$ 时，
		$
		t_n \;\Longrightarrow\; N(0,1).
		$
		直观地，
		$
		\frac{V}{n}\xrightarrow{P}1,
		\qquad
		\text{其中 } V\sim\chi^2_n.
		$
		
		\underline{Connection with Normal Samples.}
		设 $X_1,\dots,X_n$ 为来自 $N(\mu,\sigma^2)$ 的随机样本，
		$
		\bar X_n=\frac1n\sum_{i=1}^n X_i,
		\qquad
		s^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X_n)^2.
		$
		则
		$
		\frac{\sqrt n(\bar X_n-\mu)}{s}\sim t_{n-1}.
		$
		
		\underline{Key Facts.}
		\begin{itemize}
			\item $t_n$ 是对称、重尾分布；
			\item 自由度 $n$ 越大，越接近标准正态；
			\item $t$ 统计量源自未知方差的正态样本推断。
		\end{itemize}
		
		置信區間的
		
		\underline{Confidence Intervals (CIs).}
		置信区间用于量化对未知参数 $\theta$（或其函数 $g(\theta)$）的估计不确定性。
		我们希望构造一个随机区间 $(A,B)$，使其在重复抽样意义下有高概率包含真值。
		
		\underline{Definition (Level / Coefficient).}
		设 $X_n=(X_1,\dots,X_n)$ 来自分布族 $P_\theta$，其中 $\theta\in\Omega\subset\mathbb R^k$。
		欲估计 $g(\theta)$。若统计量 $A\le B$ 满足对所有 $\theta$，
		$
		\mathbb P_\theta\!\bigl(A\le g(\theta)\le B\bigr)\ge 1-\alpha,\qquad \alpha\in(0,1),
		$
		则随机区间 $(A,B)$ 称为 $g(\theta)$ 的 $(1-\alpha)$ 置信区间（置信水平/系数）。
		若上式对所有 $\theta$ 都取等号，则称该 CI 为 \emph{exact}（精确置信区间）。
		
		\underline{Standard Method: Pivots (枢轴量法).}
		构造 CI 的常用套路是找一个 \emph{pivot}（枢轴量）：
		$
		\Psi\bigl(X_n,g(\theta)\bigr),
		$
		使得其在参数为 $\theta$ 时的分布 \emph{不依赖于 $\theta$}，且分布已知。
		设该 pivot 的分布函数为 $G$，并记其 $\beta$ 分位数为 $q(G;\beta)$，即
		$
		\mathbb P_\theta\!\left(\Psi\bigl(X_n,g(\theta)\bigr)\le q(G;\beta)\right)=\beta.
		$
		选取 $0\le \beta_1,\beta_2\le \alpha$ 且 $\beta_1+\beta_2=\alpha$，则
		$
		\mathbb P_\theta\!\left(q(G;\beta_1)\le \Psi\bigl(X_n,g(\theta)\bigr)\le q(G;1-\beta_2)\right)=1-\alpha.
		$
		把上述不等式对 $g(\theta)$（或 $\theta$）解出来，得到 $(1-\alpha)$ 的置信集合/区间。
		\emph{常见选择是等分尾部：} $\beta_1=\beta_2=\alpha/2$（通常可得到最短的双侧 CI）。
		
		\underline{Example 1: Normal Mean, $\sigma$ Known.}
		若 $X_1,\dots,X_n\stackrel{\text{i.i.d.}}{\sim}N(\mu,\sigma^2)$ 且 $\sigma$ 已知，
		$
		\bar X=\frac1n\sum_{i=1}^n X_i,\qquad \bar X\sim N\!\left(\mu,\frac{\sigma^2}{n}\right).
		$
		标准化得到 pivot
		$
		\Psi(X_n,\mu)=\frac{\bar X-\mu}{\sigma/\sqrt n}=\frac{\sqrt n(\bar X-\mu)}{\sigma}\sim N(0,1).
		$
		令 $z_\beta$ 为 $N(0,1)$ 的 \emph{upper} $\beta$ 分位数（$\mathbb P(Z>z_\beta)=\beta$），
		则双侧 $(1-\alpha)$ CI 满足
		$
		\mathbb P_\mu\!\left(-z_{\alpha/2}\le \frac{\sqrt n(\bar X-\mu)}{\sigma}\le z_{\alpha/2}\right)=1-\alpha,
		$
		等价于
		$
		\mathbb P_\mu\!\left(\bar X-\frac{\sigma}{\sqrt n}z_{\alpha/2}\le \mu\le
		\bar X+\frac{\sigma}{\sqrt n}z_{\alpha/2}\right)=1-\alpha,
		$
		因此
		$
		\mathrm{CI}_{1-\alpha}(\mu)=\left[\bar X-\frac{\sigma}{\sqrt n}z_{\alpha/2},\ \bar X+\frac{\sigma}{\sqrt n}z_{\alpha/2}\right].
		$
		该 CI 长度为
		$
		L=2\frac{\sigma}{\sqrt n}z_{\alpha/2}.
		$
		\emph{权衡：} $\alpha$ 越小（置信度越高），$z_{\alpha/2}$ 越大，区间越宽；固定 $\alpha$ 时，
		$n$ 越大，长度按 $1/\sqrt n$ 缩短（样本量增至 $4$ 倍，长度减半）。
		
		\underline{Example 2: Normal Mean, $\sigma$ Unknown.}
		若仍有 $X_i\stackrel{\text{i.i.d.}}{\sim}N(\mu,\sigma^2)$ 但 $\sigma$ 未知，
		$
		s^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X)^2.
		$
		直接用 $\sigma$ 的 pivot 不可行（含 nuisance 参数）。用 $s$ 替代 $\sigma$：
		$
		\Psi(X_n,\mu)=\frac{\sqrt n(\bar X-\mu)}{s}.
		$
		利用正态样本性质可得该 pivot 服从 $t$ 分布（自由度 $n-1$）：
		$
		\frac{\sqrt n(\bar X-\mu)}{s}\sim t_{n-1}.
		$
		由对称性，设 $q(t_{n-1};1-\alpha/2)$ 为上 $1-\alpha/2$ 分位数，则
		$
		\mathbb P_{\mu,\sigma^2}\!\left(-q(t_{n-1};1-\alpha/2)\le \frac{\sqrt n(\bar X-\mu)}{s}\le
		q(t_{n-1};1-\alpha/2)\right)=1-\alpha,
		$
		等价于
		$
		\mathrm{CI}_{1-\alpha}(\mu)=\left[\bar X-\frac{s}{\sqrt n}q(t_{n-1};1-\alpha/2),\ 
		\bar X+\frac{s}{\sqrt n}q(t_{n-1};1-\alpha/2)\right].
		$
		
		\underline{Asymptotic Pivots via CLT (大样本近似).}
		若 $X_1,\dots,X_n$ 为来自某分布 $F$ 的 i.i.d. 样本，满足
		$
		\mathbb E[X_1]=\mu,\qquad \mathrm{Var}(X_1)=\sigma^2,
		$
		则由 CLT，
		$
		\frac{\sqrt n(\bar X-\mu)}{\sigma}\ \overset{\text{approx}}{\sim}\ N(0,1)\qquad (n\ \text{大}).
		$
		若 $\sigma$ 已知，则得到近似 CI：
		$
		\mu \approx \bar X \pm \frac{\sigma}{\sqrt n}z_{\alpha/2}.
		$
		实际中 $\sigma$ 常未知，可用 $s$ 替代，仍得到近似 CI：
		$
		\mu \approx \left(\bar X-\frac{s}{\sqrt n}z_{\alpha/2},\ \bar X+\frac{s}{\sqrt n}z_{\alpha/2}\right),
		$
		但其真实覆盖率可能偏离 $1-\alpha$，并依赖于 $F$ 的形状与样本量 $n$。
		
		\underline{Exercise (Bernoulli, Large $n$).}
		若 $X_i\stackrel{\text{i.i.d.}}{\sim}\mathrm{Bernoulli}(\theta)$，则
		$
		\mathbb E[X_1]=\theta,\qquad \mathrm{Var}(X_1)=\theta(1-\theta).
		$
		令样本比例 $\hat\theta=\frac1n\sum_{i=1}^n X_i$，大样本下可用近似 pivot 构造 CI，得到（题中形式）
		$
		\left[\hat\theta-\sqrt{\frac{\hat\theta(1-\hat\theta)}{\,n-1\,}}\,z_{\alpha/2},\ 
		\hat\theta+\sqrt{\frac{\hat\theta(1-\hat\theta)}{\,n-1\,}}\,z_{\alpha/2}\right]
		$
		为 $\theta$ 的近似 $(1-\alpha)$ CI。
		
		\underline{Interpretation (如何正确解读).}
		令 $(A,B)$ 为参数 $\theta$ 的置信系数 $\gamma$（即 $\gamma=1-\alpha$）的置信区间，
		观察到具体数值 $(a,b)$ 后：
		\begin{itemize}
			\item \textbf{错误说法：} “$\theta$ 以概率 $\gamma$ 落在 $(a,b)$ 里。”
			\item \textbf{正确频率学派含义：} 在重复抽样下，随机区间 $(A(X_n),B(X_n))$ 以概率 $\gamma$
			覆盖真值 $\theta$：
			$
			\mathbb P_\theta\!\bigl(A(X_n)\le \theta\le B(X_n)\bigr)=\gamma.
			$
			\item 观测到 $(a,b)$ 之后，若不把 $\theta$ 当随机变量，就不能对事件 $\{\theta\in(a,b)\}$ 再赋“概率”；
			通常表述为“我们对 $\theta\in(a,b)$ 有置信度 $\gamma$”。
		\end{itemize}
		
		
		
		\textbf{C-R Ineq}
		\section*{Cramér--Rao Inequality (Cheat Sheet)}
		
		\underline{Model \& Notation}
		
		Let $X\sim f(x,\theta)$, $\theta\in\Omega$.
		
		$
		\ell(x,\theta)=\log f(x,\theta), \qquad 
		\dot\ell(x,\theta)=\frac{\partial}{\partial\theta}\ell(x,\theta).
		$
		
		For i.i.d.\ samples $X_1,\dots,X_n$,
		$
		\ell_n(X_n,\theta)=\sum_{i=1}^n \ell(X_i,\theta), \qquad
		\dot\ell_n(X_n,\theta)=\sum_{i=1}^n \dot\ell(X_i,\theta).
		$
		
		---
		
		\underline{Fisher Information}
		
		Single observation:
		$
		I(\theta)=\mathbb E_\theta\!\left[\dot\ell(X,\theta)^2\right].
		$
		
		$n$ observations:
		$
		I_n(\theta)=nI(\theta).
		$
		
		Equivalent form (under regularity conditions):
		$
		I(\theta)=-\mathbb E_\theta\!\left[\ddot\ell(X,\theta)\right].
		$
		
		---
		
		\underline{Cramér--Rao Inequality}
		
		Let $T(X_n)$ be an unbiased estimator of $g(\theta)$, i.e.,
		$
		\mathbb E_\theta[T(X_n)]=g(\theta).
		$
		
		Then,
		$
		\boxed{
			\mathrm{Var}_\theta(T(X_n))
			\;\ge\;
			\frac{[g'(\theta)]^2}{I_n(\theta)}
		}
		$
		
		In particular, for $g(\theta)=\theta$,
		$
		\boxed{
			\mathrm{Var}_\theta(T)\ge \frac{1}{I_n(\theta)}
		}
		$
		
		---
		
		\underline{Interpretation}
		
		\begin{itemize}
			\item $I(\theta)$ measures the amount of information about $\theta$ in the data.
			\item Large $I(\theta)$ $\Rightarrow$ small variance lower bound.
			\item Small $I(\theta)$ $\Rightarrow$ no unbiased estimator can be precise.
		\end{itemize}
		
		If equality holds, $T$ is called an \emph{efficient estimator} and is the
		\emph{minimum variance unbiased estimator (MVUE)}.
		
		---
		
		\underline{Example 1: Poisson$(\theta)$}
		
		Let $X_1,\dots,X_n \sim \text{Pois}(\theta)$.
		
		$
		\dot\ell(x,\theta)=-1+\frac{x}{\theta},
		\qquad
		I_n(\theta)=\frac{n}{\theta}.
		$
		
		Cramér--Rao bound:
		$
		\mathrm{Var}(T)\ge \frac{\theta}{n}.
		$
		
		Since $\bar X$ is unbiased and $\mathrm{Var}(\bar X)=\theta/n$,
		$
		\bar X \text{ is the MVUE of } \theta.
		$
		
		---
		
		\underline{Example 2: Normal$(0,V)$}
		
		Let $X_1,\dots,X_n \sim N(0,V)$.
		
		$
		I_n(V)=\frac{n}{2V^2}.
		$
		
		Cramér--Rao bound:
		$
		\mathrm{Var}(S)\ge \frac{2V^2}{n}.
		$
		
		Consider $\hat V=\frac1n\sum_{i=1}^n X_i^2$.
		Then,
		$
		\mathrm{Var}(\hat V)=\frac{2V^2}{n}.
		$
		
		Hence,
		$
		\hat V \text{ is the MVUE of } V.
		$

		
		\underline{Key Takeaway}
		
		\begin{center}
			\emph{Cramér--Rao gives the best possible variance for unbiased estimators.  
				If the bound is attained, the estimator is optimal (MVUE).}
		\end{center}
		
