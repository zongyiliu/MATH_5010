\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{bbold}
\usepackage{fvextra, csquotes}

\title{Pratice Problems }

\author{Fangyi Chen}
\date{}


\begin{document}
\maketitle
Dec 4, 2025

\section*{Problem 1(a)}
Suppose that $X$ follows a geometric distribution,

$$
P(X=k)=p(1-p)^{k}, \quad k=0,1, \ldots,
$$

and assume that we are given an i.i.d. sample $X_{1}, \ldots, X_{n}$.\\
(a) Find the Fisher Information in a single random variable.

\begin{itemize}
  \item $f(x, \theta)$ is the pmf/pdf of $X$
  \item (original) $I(\theta)=\mathbb{E}_{\theta}\left(\dot{\ell}(X, \theta)^{2}\right)$, where $\dot{\ell}(X, \theta)=\frac{\partial}{\partial \theta} \log f(X, \theta)$.
  \item (alternative) $I(\theta)=-\mathbb{E}_{\theta}(\ddot{\ell}(X, \theta)), \ddot{\ell}(X, \theta)=\frac{\partial^{2}}{\partial \theta^{2}} \log f(X, \theta)$
\end{itemize}

\section*{Problem 1(a)}
\section*{Solution:}
\begin{itemize}
  \item $\dot{\ell}(X, p)=\frac{\partial}{\partial p} \log p(1-p)^{X}=\frac{1}{p}-\frac{X}{1-p}$.
  \item $\ddot{\ell}(X, p)=\frac{\partial^{2}}{\partial p^{2}} \log p(1-p)^{X}=-\frac{1}{p^{2}}-\frac{X}{(1-p)^{2}}$.
  \item $I(p)=\mathbb{E}_{\theta}\left(\dot{\ell}(X, p)^{2}\right)=\operatorname{Var}(\dot{\ell}(X, p))=\operatorname{Var}\left(\frac{X}{1-p}\right)= \frac{1}{(1-p)^{2}} \cdot \frac{1-p}{p^{2}}=\frac{1}{p^{2}(1-p)}$.
  \item $I(p)=-\mathbb{E}_{\theta}(\ddot{\ell}(X, p))=\frac{1}{p^{2}}+\frac{1}{(1-p)^{2}} \mathbb{E}(X)=\frac{1}{p^{2}}+\frac{1}{(1-p)^{2}} \cdot \frac{1-p}{p}= \frac{1}{p^{2}(1-p)}$.
\end{itemize}

Problem 1(b)

Suppose that $X$ follows a geometric distribution,

$$
P(X=k)=p(1-p)^{k}, \quad k=0,1, \ldots,
$$

and assume that we are given an i.i.d. sample $X_{1}, \ldots, X_{n}$.\\
(b) Find the method of moments estimator of $p$.

\section*{Solution:}
\begin{itemize}
  \item We only need one equation to get the estimator (only one parameter $p$ ).
  \item $\mathbb{E}(X)=\frac{1-p}{p}$. Let $\bar{X}=\frac{1-p}{p}$, we have $\hat{p}_{\text {MOM }}=\frac{1}{1+\bar{X}}$.
\end{itemize}

Problem 1(c)\\
Suppose that $X$ follows a geometric distribution,

$$
P(X=k)=p(1-p)^{k}, \quad k=0,1, \ldots,
$$

and assume that we are given an i.i.d. sample $X_{1}, \ldots, X_{n}$.\\
(c) Find the MLE of $p$.

\section*{Solution:}
\begin{itemize}
  \item Likelihood: $L_{n}(p)=\prod_{i=1}^{n} p(1-p)^{X_{i}}$.
  \item Log-likelihood: $\log L_{n}(p)=\sum_{i=1}^{n}\left(\log p+X_{i} \log (1-p)\right)$.
  \item Let $\frac{\partial}{\partial p} \log L_{n}(p)=0$, we have $\frac{n}{p}-\frac{\sum_{i=1}^{n} X_{i}}{1-p}=0 \Rightarrow \hat{p}_{\text {MLE }}=\frac{1}{1+\bar{X}}$.
  \item Check: $\frac{\partial^{2}}{\partial p^{2}} \log L_{n}(p)=-\frac{n}{p}-\frac{\sum_{i=1}^{n} X_{i}}{(1-p)^{2}}<0$.
\end{itemize}

\section*{Problem 1(d)}
Suppose that $X$ follows a geometric distribution,

$$
P(X=k)=p(1-p)^{k}, \quad k=0,1, \ldots,
$$

and assume that we are given an i.i.d. sample $X_{1}, \ldots, X_{n}$.\\
(d) Find the asymptotic distribution of the MLE.

\section*{Solution:}
$$
\sqrt{n}\left(\hat{p}_{\mathrm{MLE}}-p\right) \xrightarrow{d} \mathcal{N}\left(0, I(p)^{-1}\right)=\mathcal{N}\left(0, p^{2}(1-p)\right) .
$$

\section*{Problem 1(e)}
Suppose that $X$ follows a geometric distribution,

$$
P(X=k)=p(1-p)^{k}, \quad k=0,1, \ldots,
$$

and assume that we are given an i.i.d. sample $X_{1}, \ldots, X_{n}$.\\
(e) Let $p$ have a uniform prior distribution on $[0,1]$. What is the posterior distribution of $p$ ? What is the posterior mean?

\section*{Concept:}
\begin{itemize}
  \item Joint density function: $f\left(x_{1} \mid \theta\right) \ldots f\left(x_{n} \mid \theta\right)$ (function of $\left.x_{1}, \ldots, x_{n}\right)$
  \item Prior distribution: $\xi(\theta)$ (function of $\theta$ )
  \item Posterior distribution: $\xi(\theta \mid \boldsymbol{x})=\frac{f\left(x_{1} \mid \theta\right) \ldots f\left(x_{n} \mid \theta\right) \xi(\theta)}{\int f\left(x_{1} \mid \theta\right) \ldots f\left(x_{n} \mid \theta\right) \xi(\theta) \mathrm{d} \theta}$ (function of $\theta$ )
\end{itemize}

Problem 1(e)

\begin{itemize}
  \item $\xi(\theta \mid \boldsymbol{x})$ is a density function of $\theta$ since
\end{itemize}

$$
\int \frac{f\left(x_{1} \mid \theta\right) \ldots f\left(x_{n} \mid \theta\right) \xi(\theta)}{\int f\left(x_{1} \mid \theta\right) \ldots f\left(x_{n} \mid \theta\right) \xi(\theta) \mathrm{d} \theta} \mathrm{~d} \theta=1 .
$$

\begin{itemize}
  \item It might not be easy to calculate the normalizing constant $\int f\left(x_{1} \mid \theta\right) \ldots f\left(x_{n} \mid \theta\right) \xi(\theta) \mathrm{d} \theta$, but in many cases we do not need to calculate it.
  \item Common distribution families:
  \item Gamma: $\frac{\lambda^{\alpha}}{\Gamma(\alpha)} x^{\alpha-1} e^{-\lambda x} 1_{(0, \infty)}(x)$
  \item Beta: $\frac{1}{B(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1} \mathbb{1}_{(0,1)}(x)$
  \item Normal: $\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right) 1_{(-\infty,+\infty)}(x)$
  \item Once the red part and the orange part are determined, the blue part (normalizing constant) is also determined.
\end{itemize}

Problem 1(e)

\begin{itemize}
  \item $\xi(\theta \mid \boldsymbol{x})=\frac{f\left(x_{1} \mid \theta\right) \ldots f\left(x_{n} \mid \theta\right) \xi(\theta)}{\int f\left(x_{1} \mid \theta\right) \ldots f\left(x_{n} \mid \theta\right) \xi(\theta) \mathrm{d} \theta} \propto f\left(x_{1} \mid \theta\right) \ldots f\left(x_{n} \mid \theta\right) \xi(\theta)$.
  \item $f\left(x_{1} \mid p\right) \ldots f\left(x_{n} \mid p\right)=\prod_{i=1}^{n} p(1-p)^{X_{i}}$.
  \item $\xi(p)=\mathbf{1}_{(0,1)}(p)$.
  \item $\xi(p \mid \boldsymbol{x}) \propto f\left(x_{1} \mid p\right) \ldots f\left(x_{n} \mid p\right) \xi(p)=p^{n}(1-p)^{\sum_{i=1}^{n} X_{i}} \mathbb{1}_{(0,1)}(x)$.
  \item $\xi(p \mid x) \sim \operatorname{Beta}\left(n+1, \sum_{i=1}^{n} X_{i}+1\right)$
  \item Posterior mean $\mathbb{E} \xi(p \mid \boldsymbol{x})=\frac{n+1}{n+1+\sum_{i=1}^{n} X_{i}+1}$.
\end{itemize}

\section*{Problem 2}
Suppose that $Y \sim N\left(0, \sigma^{2}\right)$, where $\sigma^{2}$ is unknown. What is the distribution of $Y^{2} / \sigma^{2}$. Use the pivotal quantity $Y^{2} / \sigma^{2}$ to find:\\
(a) a $95 \%$ confidence interval for $\sigma^{2}$.\\
(b) a $95 \%$ upper confidence limit for $\sigma^{2}$.\\
(c) a $95 \%$ lower confidence limit for $\sigma^{2}$.

\section*{Problem 2}
(a) Observe that $Y^{2} / \sigma^{2} \sim \chi_{1}^{2}$. Thus,

$$
\begin{aligned}
0.95 & =\mathbb{P}\left(q\left(\chi_{1}^{2}, 0.025\right) \leq \frac{Y^{2}}{\sigma^{2}} \leq q\left(\chi_{1}^{2}, 0.975\right)\right) \\
& =\mathbb{P}\left(\frac{Y^{2}}{q\left(\chi_{1}^{2}, 0.975\right)} \leq \sigma^{2} \leq \frac{Y^{2}}{q\left(\chi_{1}^{2}, 0.025\right)}\right) .
\end{aligned}
$$

(b)

$$
0.95=\mathbb{P}\left(q\left(\chi_{1}^{2}, 0.05\right) \leq \frac{Y^{2}}{\sigma^{2}}\right)=\mathbb{P}\left(\sigma^{2} \leq \frac{Y^{2}}{q\left(\chi_{1}^{2}, 0.05\right)}\right) .
$$

(c)

$$
0.95=\mathbb{P}\left(\frac{Y^{2}}{\sigma^{2}} \leq q\left(\chi_{1}^{2}, 0.95\right)\right)=\mathbb{P}\left(\sigma^{2} \geq \frac{Y^{2}}{q\left(\chi_{1}^{2}, 0.95\right)}\right) .
$$

\section*{Problem 3}
Let $Y_{1}, \ldots, Y_{n}$ denote a random sample of size $n$ from a population with a uniform distribution on the interval $[0, \theta]$. Let $Y_{(n)}=\max \left\{Y_{1}, \ldots Y_{n}\right\}$. Let $U=(1 / \theta) Y_{(n)}$.\\
(a) Find the distribution function of $U$.\\
(b) Is $U$ a pivotal quantity? Use this, or otherwise, find a $95 \%$ lower confidence bound for $\theta$.

Pivotal quantity: a random variable whose distribution does not depend on $\theta$.

\section*{Problem 3}
(a) Let $G_{n}$ be the c.d.f. of $Y_{(n)}$. Then,

$$
\begin{gathered}
G_{n}(v)=\left(\frac{v}{\theta}\right)^{n}, \quad 0 \leq v \leq \theta . \\
F_{U}(u)= \begin{cases}0, & u \leq 0 \\
u^{n}, & 0 \leq u \leq 1 \\
1, & u>1 .\end{cases}
\end{gathered}
$$

(b) As the distribution of $U$ does not depend on $\theta, U$ is a pivot. Note that

$$
0.95=F_{U}(u)=u^{n} \quad \Rightarrow \quad u=(0.95)^{1 / n} \text {. }
$$

Thus,

$$
0.95=\mathbb{P}\left(\frac{Y_{(n)}}{\theta}<(0.95)^{1 / n}\right)=\mathbb{P}\left(\theta>\frac{Y_{(n)}}{(0.95)^{1 / n}}\right) .
$$

Therefore, a $95 \%$ lower confidence bound for $\theta$ is $\frac{Y_{(n)}}{(0.95)^{1 / n}}$.

\section*{Problem 4(a)}
Let $X_{1}, \ldots, X_{n}$ be a random sample from the $\operatorname{Uniform}(\theta, \theta+1)$ distribution. To test $H_{0}: \theta=0$ versus $H_{1}: \theta>0$, use the test

\begin{displayquote}
reject $H_{0}$ if $Y_{n} \geq 1$ or $Y_{1} \geq k$,
\end{displayquote}

where $k$ is a constant, $Y_{1}=\min \left\{X_{1}, \ldots, X_{n}\right\}, Y_{n}=\max \left\{X_{1}, \ldots, X_{n}\right\}$.\\
(a) Determine $k$ so that the test will have size $\alpha$.

\section*{Concept:}
\begin{itemize}
  \item $H_{0}: \theta \in \Omega_{0} \quad$ v.s. $H_{1}: \theta \in \Omega_{1}$
  \item Power function: $\pi(\theta \mid \delta)=\mathbb{P}_{\theta}\left(\delta\right.$ reject $\left.H_{0}\right) \quad$ (no matter it is right or wrong).
  \item Size: $\alpha(\delta)=\sup _{\theta \in \Omega_{0}} \pi(\theta \mid \delta)$.
\end{itemize}

\section*{Problem 4(a)}
Under $H_{0}$, we have $\theta=0$. Thus, $X_{i} \sim \operatorname{Uniform}(0,1)$. Rejection region: Reject $H_{0}$ if $Y_{n} \geq 1$ or $Y_{1} \geq k$. Therefore,

$$
\alpha=\mathbb{P}_{\theta=0}\left(Y_{n} \geq 1 \cup Y_{1} \geq k\right) .
$$

Since $\mathbb{P}_{\theta=0}\left(Y_{n} \geq 1\right)=0$, the expression simplifies to $\alpha=\mathbb{P}_{\theta=0}\left(Y_{1} \geq k\right)$.

$$
\begin{aligned}
\alpha=\mathbb{P}_{\theta=0}\left(Y_{1} \geq k\right) & =\mathbb{P}\left(X_{1} \geq k, X_{2} \geq k, \ldots, X_{n} \geq k\right) \\
& =\left(\int_{k}^{1} 1 d x\right)^{n} \\
& =(1-k)^{n} .
\end{aligned}
$$

Hence $k=1-\alpha^{1 / n}$.

\section*{Problem 4(b)}
Let $X_{1}, \ldots, X_{n}$ be a random sample from the $\operatorname{Uniform}(\theta, \theta+1)$ distribution. To test $H_{0}: \theta=0$ versus $H_{1}: \theta>0$, use the test

$$
\text { reject } H_{0} \text { if } Y_{n} \geq 1 \text { or } Y_{1} \geq k \text {, }
$$

where $k$ is a constant, $Y_{1}=\min \left\{X_{1}, \ldots, X_{n}\right\}, Y_{n}=\max \left\{X_{1}, \ldots, X_{n}\right\}$.\\
(b) Find an expression for the power function of the test in part (a).

For given $\theta, X_{i} \sim \operatorname{Uniform}(\theta, \theta+1)$.

$$
\pi(\theta \mid \delta)=\mathbb{P}_{\theta}\left(\text { Reject } H_{0}\right)=1-\mathbb{P}_{\theta}\left(\text { Do not reject } H_{0}\right)
$$

The condition "Do not reject" corresponds to: $Y_{n}<1$ and $Y_{1}<k$. This implies all $X_{i}<1$ and at least one $X_{i}<k$ (equivalent to min $X_{i}<k$ ).

Problem 4(b)\\
Case 1: $\theta \geq k$\\
The support of the distribution is $[\theta, \theta+1]$. Since $\theta \geq k$, all observed values $X_{i} \geq k$. Therefore, $Y_{1} \geq k$ is always true. $\pi(\theta \mid \delta)=1$.

Case 2: $0<\theta<k$\\
We calculate the probability of the acceptance region $A=\left\{\right.$ all $X_{i}<1$ and $\left.Y_{1}<k\right\}$. For all $X_{i}$ to be less than 1 , they must fall in the interval $[\theta, 1)$.

$$
\begin{aligned}
\mathbb{P}(A) & =\mathbb{P}\left(\text { all } X_{i}<1\right)-\mathbb{P}\left(\text { all } X_{i}<1 \text { and } Y_{1} \geq k\right) \\
& =(1-\theta)^{n}-(1-k)^{n}
\end{aligned}
$$

The power is $1-P(A)$ :

$$
\pi(\theta \mid \delta)=1-P(A)=1-(1-\theta)^{n}+(1-k)^{n},
$$

where $k=1-\alpha^{1 / n}$.

\section*{Problem 5}
In each of the following situations, calculate the p -value of the observed data.\\
(a) For testing $H_{0}: \theta \leq 1 / 2$ versus $H_{1}: \theta>1 / 2,7$ successes are observed out of 10 Bernoulli trials.\\
(b) For testing $H_{0}: \lambda \leq 1$ versus $H_{1}: \lambda>1, X=3$ is observed where $X \sim \operatorname{Poisson}(\lambda)$.\\
(c) For testing $H_{0}: \theta=1 / 2$ versus $H_{1}: \theta \neq 1 / 2,7$ successes are observed out of 10 Bernoulli trials.

\section*{Concept:}
\begin{itemize}
  \item P-value: smallest level $\alpha$ such that we would reject $H_{0}$ at level $\alpha$ with the observed data.
  \item (alternative): The probability that the test statistic is at least "extreme" as the observed one.
\end{itemize}

\section*{Problem 5}
\begin{itemize}
  \item $X \sim \operatorname{Binomial}(n=10, \theta)$, observed $x=7$.
\end{itemize}

$$
p=\mathbb{P}_{\theta=1 / 2}(X \geq 7)=\sum_{k=7}^{10}\binom{10}{k}\left(\frac{1}{2}\right)^{10}=\frac{176}{1024} \approx 0.172 .
$$

The reason of fixing $\theta=1 / 2$ : Among $\Omega_{0}, \theta=1 / 2$ has the largest power function if the rejection region is $\{X \geq k\}$ for $k \geq 5$.

\begin{itemize}
  \item $X \sim \operatorname{Poisson}(\lambda)$, observed $x=3$.
\end{itemize}

$$
\begin{aligned}
p=\mathbb{P}_{\lambda=1}(X \geq 3) & =1-\mathbb{P}_{\lambda=1}(X \leq 2) \\
& =1-e^{-1}\left(1+1+\frac{1}{2}\right)=1-\frac{5}{2 e} \approx 0.0803 .
\end{aligned}
$$

\begin{itemize}
  \item $X \sim \operatorname{Binomial}(n=10, \theta)$, observed $x=7$.
\end{itemize}

$$
p=\mathbb{P}_{\theta=1 / 2}(X \geq 7 \text { and } X \leq 3)=2 \times 0.172=0.344 .
$$


\end{document}
